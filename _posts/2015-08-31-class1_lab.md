---
layout: post
title:  "Lab 1: The Data Science Toolkit"
date:   2015-08-31 19:00:00
meeting: "1"
notes: ""
#readings: "There are no readings for today."
#assignment: "There is no assignment for today."
permalink: /classes/class1lab/
categories: "welcome tools"
header-img: "img/1_header.png"


---
## The Data Science Development Environment
When learning and doing data science, the *development environment* becomes the platform that you have to process and analyze data. The term comes from applications development in which development changes are typically moved from a development machine through to a testing environment and eventually to the production application. While traditionally statistics research has been developed and analyzed within a single local machine and maybe a single program, increasingly analytics apps are deployed in such a way that the data is *actively* used and processing of the data employs a wide variety of packages and programs.  This can include use in a dashboard,  can be deployed within a production environment, providing such things as forecasts or pricing changes that are directly acted upon in a transactional system.  In these types of contexts, there is good reason to try to make the development environment as close to the production environment as possible and to employ techniques.  Similar systems across development and production help reduce the possibility that something will go wrong.
	
When learning analytics, a similar development environment can ensure that associated examples and exercises will work across a population of users-who may be using a wide variety of different types of operating systems. Whenever you develop modules your code will often rely upon packages that have been written by others.  These packages can provide tremendous functionality while abstracting out complexities that may not be necessary for you to undertake.  For example, packages to enable downloading of data from the Twitter Stream API or conducting a neural network analysis, but when then can be accessed through calling a specific class this can enable the complexity to be hidden.  It could be that such packages further have incorporated packages.  Such packages necessarily may need to be directly compatible with the operating system.  Further complicating the matter  the packages often have versioning with specific apps sets of code potentially conflicting with required apps.
	
This ended up being a significant issue for me the first analytics class I taught a few years ago.  I was trying to provide the students with experience interacting with a number of databases in R.  Extracting data from databases is not that complex when you use some of the available packages that handle the connection to the database.  However, I found that while the exercises has worked just fine on my Mac, it didn't work well on anyone who had a windows machine because the versions available for each operating system were different. You similarly don't want those types of issues if implementation some type of analytics based production environment. 

For that reason, we will be incorporating a number of foundational technology tools likely to be important for the data scientist working in the wild as part of the education process.  They include *Git/GitHub*, *virtual machines*,  and *the Linux command line*.

### Git/Github
Git and Github provide a Version Control System (VCS) and collaboration platform by which you can control versions of analyses (which are essentially programs) as well as the underlying platform for doing analyses.  Git is a program that is installed on your computer that can clone a publicly available *repository* of code and transfer that information to your machine.  Github is the most popular system for sharing and managing code.  They also have an easy to use graphical user interface (GUI) for Git called GitHub called *Github Desktop* that makes managing repositories easy. 

Let's say you have an analysis pipeline that pulls sales data from your CRM and uses it in a forecasting dashboard for your production department.  That process needs consistency, such that changes to the code used to make the prediction are tracked and controlled.  If you make and update to the algorithm or the way the data is incorporated into the analysis on your local machine you can *commit* it to the repository to make sure that others also have access to the change.  By committing to the repository and synching your changes with Github, everyone working around this project can download this change, and that there is a clear way to go back if anything goes wrong.  

Git/Github also gives you the capability to share and test a change without committing it your production environment.  Let's say you would like to experiment with a new prediction algorithm, share and test the changes but not move them into production.  Git makes this easy through the concept of *branching*.  The *master* branch is typically the main production copy of your code.  Creating a new branch keeps your changes separate from the production branch.  You can *commit* your changes to the new branch and then others could download them and test them.  If everything looked good and worked appropriately, you can merge the branch into the production branch.  

*Note. When using Git, start with GitHub Desktop but you should also learn your way around the command line.  You will likely find it useful if you have to make changes directly on a unix machine.*

We will get some active work in using Git as part of this lab.  

### Virtual Machines
Over the past few years there has been an emergence of some wonderful tools for configuring your local development environment for analytics, whether you are a Mac, PC, or Linux user.  The solution involves utilizing a local virtual machine as an alternative to installing and running programs on your local operating system.  This enables everyone to use the same development environment or operating system.  We will spend less time troubleshooting issues and more time doing data science.  

But *what is a virtual machine?*  A virtual machine is a secondary operating system run on top of your primary operating system. You are likely running either OS X (Mac)  or Windows as your primary operating system.  This is the *host* machine.  It is most common to utilize a virtual machine based upon one of the Linux distributions for your development environment.  The Linux machine is a *guest* of your host.  While it is entirely possible to have a Linux machine be a guest in another Linux machine, if you are using a Linux desktop as your primary computer you probably don't need me to explain this to you.  This Linux distribution is the core foundation on which we will build our analytics applications, installing any necessary dependencies and ensuring that whatever your host operating system you have a consistent environment. 

For your work in this class we recommend you use our virtual machine based on the Ubuntu Linux operating system.  This will simplify your setup time for exercises.  How will we share changes involving the virtual machine?  Well, using Github and Git of course.  However, there are a few more tools you should be familiar with that we will be using as part of our project. 

If we setup a virtual machine and shared it with the class, that could be a great way to get everyone on the same development environment.  This is the approach that SAS has taken in their virtual machine environment called [SAS University Edition](http://www.sas.com/en_us/software/university-edition.html).  

For our Python and R work, we will be using a custom virtual machine.  We could do the same thing and distribute a virtual machine.  However, every time we wanted to make an adjustment to the virtual machine we would have to re-download the entire machine (~5 GB). Not cool!  Instead we will be using some nifty tools to be able to configure a virtual machine from a "base box".

###Vagrant and Chef
[Vagrant](https://www.vagrantup.com) allows you to "Create and configure lightweight, reproducible, and portable development environments."  We want a development environment for analytics that is preconfigured to do data science, with lots of tools and capabilities build in.  

Vagrant does half of the magic, configuring the download of a base box, sharing of files from the *host* machine (your computer) to the *guest machine*, mapping of ports and ssh connections so that you can access the guest machine from your host machine.  If you want to take a look and see some of what is under the hood, you can open up the `vagrantfile` in the github repository and see what is there. 

 [Chef](https://www.chef.io) does the rest of the magic, configuring specific modules that we want to have installed.  A collaborative community around Chef has developed so that you can find *recipes* for nearly any type of program you would like to install on your development environment in the [Chef supermarket](https://supermarket.chef.io). 
 
 Whether it is the first time using your vagrant machine or not you can launch it from the command line by simply typing `vagrant up`.  This will download (if necessary) and launch the machine so that you can access it.  When you are done using it, just type `vagrant halt` into the command line and it will shutdown the machine.  
 
 *Note. We have found that some PC computers will come with support for virtualization turned off.  This is a bios setting that must be turned on.  Adjustment of bios is specific to the type of computer, so google "YOUR MODEL bios setting" and then look for a setting that involves turning virtualization on.  Usually it involves some combination of keys while you start up the machine*
 
 If you really screw up your virtual machine (no judgment, it happens) and you want to rebuild it from the base image, just type `vagrant destroy` and it will delete the appropriate files.  

We probably won't be able to get the "perfect" development environment for analytics, and there may be times when you will have to install things on your own.    You should know how to do a few things on your *guest* machine, and so let's go over the linux command line now.  

### The Linux Command Line
When using a Linux Virtual machine, we won't have the same pretty and convenient GUI that we have on our Windows or Mac device. It does exist, as we have selected the lighter-weight server version that will run faster.   You can install and run everything on a Linux Desktop machine, but this creates a lot of additional overhead on your local machine, and it doesn't easily transition to a production environment.  We will have to get used to the process of utilizing a Linux command line to do what we need to do. 

The command line will allow you to install and run programs, copy and manipulate files, change system settings, etc.  Most of the time we won't do our analytics using the command line, but it is nice to know in a pinch!

In order to enter the command line or shell of a Linux machine you will need to be in the directory of your vagrant virtual machine.  To connect to the guest machine type: `vagrant ssh`.  It should look something like this. 
![Vagrant SSH]({{ site.baseurl }}/img/1_lvagrantssh.png)

When you first ssh into your guest machine.  You will be in your home directory, which is the directory that linux has partitioned for your works.  In the case of vagrant configured machines, the user is actually called `vagrant` and the directory is `\home\vagrant`.  Wherever you are in the directory structure, you can always type `cd ` and you will return home.  To see where you are at any time, just type `pwd`. 

Files on your *host* machine in the repository directory will be mapped to the `/vagrant` directory on the guest machine.   Change to the vagrant directory with `cd /vagrant` and then use the command `ls` to show all of the files.  

There are lots of text editors available, but I'm partial to nano.  If you type `nano` *filename* you will edit a file or create it if it doesn't exist.  

To learn more about the Linux command line, check out these tutorials. 

## *Lab 1: Build the Virtual Machine for Python and R*
**Questions and issues for this lab should be posted to the Piazza site**
In order to run the virtual machine locally, we will have to do a bit of setup.  

* [Github](https://www.github.com).   If you don't yet have a Github ID, go ahead and create one now.  
* [Github Desktop](https://desktop.github.com) Github Desktop is the GUI tool to allow you to use git easily.  
* [Virtualbox](https://www.virtualbox.org/wiki/Downloads). Virtual box is the general purpose virtualizer that is able to start and maintain a virtual instance of the machine.  
* [Vagrant](http://www.vagrantup.com/downloads.html) Vagrant provides flexibility in enabling us to download a base linux distribution and then customize it to our needs.  

Once you have each of these installed, go to the [course repository](https://github.com/RPI-Analytics/MGMT6963-2015) on Github and *fork* it by pressing the ![fork]({{ site.baseurl }}/img/1_fork.png) button.  Forking allows you to experiment with the project without impacting the original project.  

Next, launch the GitHub desktop app on your local machine.   Log in using your GitHub account.  Click on `File -> Clone Repository` and type in `MGMT6963-2015` to clone the class repository to your local machine.  

Once you have successfully cloned the repository you are ready to download and install your virtual machine. 


To build the virtual machine, it is really quite easy.  You first will need to clone the repository holding the configuration files and others
`git clone alsjdlfjasdlfja`
Then just change to the directory and 
`>cd ******
>vagrant up`
The resulting configuration can take around 20 minutes depending on the size of your processes, but what is happening is really brilliant.  It starts by downloading a "base box" that can be used again should you ever need to rebuild the virtual machine.  It then uses something called [Chef](https://www.chef.io) to install the wide variety of different components necessary for doing analytics.  Bundles of these components are organized into [cookbooks]() that are in the in the Chef.io [supermarket](https://supermarket.chef.io).  Yes, the who thing is a bit silly, but it does help to immediately communicate the different pieces and how they are connected. 

Now if everything goes well, Vagrant & Chef will finish configuring your virtual machine and you are ready to get started!    

Lets first get to some of the common problems related to doing data science locally.   

we will be covering a wide variety of methods of installing and working with analytics programs. We will cover installation of R and Python locally, via a virtual machine,  via a Docker container, and on the cloud with [DominoData Labs](http://dominodatalab.com).  The virtual machine, the docker, container, and the cloud (Domino Data labs and the SAS academic cloud) get past the challenges of having different local environments by standardizing on a Linux based operating system. This ensures that the code examples work and challenges won't involve a lot of unnecessary trouble shooting.

The cloud is a wonderful solution for analytics, and you could make the case that things are just getting started compared to other areas.  Domino Data labs allows you to develop code locally and then deploy/share/run on the cloud.  This is a great way to share analyses with others and solidify repeatable findings. For startups, you can even write an API that uses the outcome of your analyses.   

When covering SAS examples, we will be using the SAS academic cloud.  This will simplify some aspects of our analysis and if you are at a college or university it is likely you will be able to gain access to the SAS cloud.  Cloud based development and deployment have the advantage of being able to standardize on a single set of available packages. 
#Download the SAS Virtual Machine


T

### Footnote: The Cloud and Analytics
Increasingly, the *cloud* has provided an excellent platform for sharing applications and even doing some targeted types of development.  When we refer to *analytics in the cloud,* this indicates that the computation and analyses are  conducted on a *server* rather than a *desktop*.  This difference is also important for setting up a development environment.  The cloud is the most stable but not always a flexible analysis environment. 
